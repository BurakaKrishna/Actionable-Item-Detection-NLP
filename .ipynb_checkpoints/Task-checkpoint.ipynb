{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actionable Email Item Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset directory, set this before your experiments\n",
    "dataset = '/home/syed.b/emails.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a sample for understanding perspective\n",
    "df_sample  = pd.read_csv(dataset, skiprows = lambda x : np.random.rand() > 0.01 and x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5328, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Message-ID: <19909580.1075855688684.JavaMail.evans@thyme>\\nDate: Fri, 1 Sep 2000 06:08:00 -0700 (PDT)\\nFrom: phillip.allen@enron.com\\nTo: mike.grigsby@enron.com, frank.ermis@enron.com\\nSubject: FYI\\nMime-Version: 1.0\\nContent-Type: text/plain; charset=us-ascii\\nContent-Transfer-Encoding: 7bit\\nX-From: Phillip K Allen\\nX-To: Mike Grigsby, Frank Ermis\\nX-cc: \\nX-bcc: \\nX-Folder: \\\\Phillip_Allen_Dec2000\\\\Notes Folders\\\\'sent mail\\nX-Origin: Allen-P\\nX-FileName: pallen.nsf\\n\\n---------------------- Forwarded by Phillip K Allen/HOU/ECT on 09/01/2000 \\n01:07 PM ---------------------------\\n   \\n\\tEnron North America Corp.\\n\\t\\n\\tFrom:  Matt Motley                           09/01/2000 08:53 AM\\n\\t\\n\\nTo: Phillip K Allen/HOU/ECT@ECT\\ncc:  \\nSubject: FYI\\n\\n\\n--\\n\\n\\n\\n - Ray Niles on Price Caps.pdf\\n\\n\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample['message'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['---------------------- Forwarded by Phillip K Allen/HOU/ECT on 02/12/2001',\n",
       " '12:18 PM ---------------------------',\n",
       " 'To: Phillip K Allen/HOU/ECT@ECT',\n",
       " 'cc:',\n",
       " 'Subject: Re: AEC Volumes at OPAL']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Establish cleaning patterns\n",
    "\n",
    "[ x.strip() for x in df_sample['message'][11].split('FileName:')[1].split('\\n',1)[1].strip('\\n').split('\\n') if x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the whole data\n",
    "\n",
    "df = pd.read_csv(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(517401, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store emails here in the list\n",
    "# Use nltk's sentence tokenizer to tokenize the email sentences -- since we notice above that \n",
    "# order\n",
    "emails = []\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    \n",
    "    emails.append(nltk.sent_tokenize(' '.join([ x.strip() for x in df['message'][i].split('FileName:')[1].split('\\n',1)[1].strip('\\n').split('\\n') if x]\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Traveling to have a business meeting takes the fun out of the trip.',\n",
       " 'Especially if you have to prepare a presentation.',\n",
       " 'I would suggest holding the business plan meetings here then take a trip without any formal business meetings.',\n",
       " 'I would even try and get some honest opinions on whether a trip is even desired or necessary.',\n",
       " 'As far as the business meetings, I think it would be more productive to try and stimulate discussions across the different groups about what is working and what is not.',\n",
       " 'Too often the presenter speaks and the others are quiet just waiting for their turn.',\n",
       " 'The meetings might be better if held in a round table discussion format.',\n",
       " 'My suggestion for where to go is Austin.',\n",
       " \"Play golf and rent a ski boat and jet ski's.\",\n",
       " 'Flying somewhere takes too much time.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity check\n",
    "emails[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Filter\n",
    "\n",
    "\n",
    "Check for action words. A curated list of action words is specified and if our sentences from the emails contain one of these words, we store it for a further filtering process.\n",
    "\n",
    "The action words file is in the current directory as this Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in action verbs\n",
    "action_verbs = []\n",
    "\n",
    "with open('./action_words.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        action_verbs.append(line.strip().lower())\n",
    "        \n",
    "        \n",
    "# Storing the actionable items as a tuple of 3 items - (text, action-words, no. of action-words in sentence)\n",
    "action_texts = []   \n",
    "\n",
    "# Also store non-actionable items.\n",
    "non_action_texts = []\n",
    "\n",
    "# Due to memory/time constraints, we do not read in the entire emails, rather take a subset of 1,00,000 to show results \n",
    "for item in emails[:100000]:\n",
    "    for text in item:\n",
    "        temp_verb_list = []\n",
    "        \n",
    "        for verb in action_verbs:\n",
    "            if verb in text.lower().split():\n",
    "                temp_verb_list.append(verb)\n",
    "        if len(temp_verb_list) > 0:\n",
    "            action_texts.append((text, temp_verb_list, len(temp_verb_list)))\n",
    "        elif len(temp_verb_list) == 0:\n",
    "            non_action_texts.append(text)\n",
    "                \n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I would suggest holding the business plan meetings here then take a trip without any formal business meetings.',\n",
       "  ['plan'],\n",
       "  1),\n",
       " ('As far as the business meetings, I think it would be more productive to try and stimulate discussions across the different groups about what is working and what is not.',\n",
       "  ['think'],\n",
       "  1),\n",
       " ('My suggestion for where to go is Austin.', ['go', 'suggestion'], 2),\n",
       " (\"Play golf and rent a ski boat and jet ski's.\", ['play'], 1),\n",
       " ('Randy, Can you send me a schedule of the salary and level of everyone in the scheduling group.',\n",
       "  ['send'],\n",
       "  1),\n",
       " ('Plus your thoughts on any changes that need to be made.', ['need'], 1),\n",
       " ('Please cc the following distribution list with updates: Phillip Allen (pallen@enron.com) Mike Grigsby (mike.grigsby@enron.com) Keith Holst (kholst@enron.com) Monique Sanchez Frank Ermis John Lavorato Thank you for your help Phillip Allen',\n",
       "  ['help', 'please'],\n",
       "  2),\n",
       " (\"1. login:  pallen pw: ke9davis I don't think these are required by the ISP 2.  static IP address IP: 64.216.90.105 Sub: 255.255.255.248 gate: 64.216.90.110 DNS: 151.164.1.8 3.\",\n",
       "  ['required', 'think'],\n",
       "  2),\n",
       " ('To do > so I need a corresponding term gas price for same.', ['need'], 1),\n",
       " ('In doing so,  I need your > best fixed price forward gas price deal for 1, 3, 5, 7 and 10 years for > annual/seasonal supply to microturbines to generate fixed kWh for > customer.',\n",
       "  ['need'],\n",
       "  1),\n",
       " ('kWh deal must have limited/ > no risk forward gas price to make deal work.',\n",
       "  ['make'],\n",
       "  1),\n",
       " ('> > We are proposing installing 180 - 240 units across a large number of > stores (60-100) in San Diego.',\n",
       "  ['proposing'],\n",
       "  1),\n",
       " ('> > For 6-8 hours a day  Microturbine run time: > Gas requirement for 180 microturbines 227 - 302 MMcf per year > Gas requirement for 240 microturbines 302 - 403 MMcf per year > > Gas will likely be consumed from May through September, during peak > electric period.',\n",
       "  ['run'],\n",
       "  1),\n",
       " ('> Gas price required: Burnertip price behind (LDC) San Diego Gas & Electric > Need detail breakout of commodity and transport cost (firm or > interruptible).',\n",
       "  ['need'],\n",
       "  1),\n",
       " ('> > Should you have additional questions, give me a call.', ['give'], 1),\n",
       " ('I have forwarded your request to Zarin Imam at EES.', ['request'], 1),\n",
       " ('Lucy, Here are the rentrolls: Open them and save in the rentroll folder.',\n",
       "  ['open'],\n",
       "  1),\n",
       " (\"Follow these steps so you don't misplace these files.\",\n",
       "  ['follow', 'steps'],\n",
       "  2),\n",
       " ('Click on Save As 2.', ['click'], 1),\n",
       " ('Click on the drop down triangle under Save in: 3.', ['click'], 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_texts[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Therein comes Sempra energy > gas trading, truly you.',\n",
       " '> Store number varies because of installation hurdles face at small percent.',\n",
       " '> Let me assure you, this is real deal!!',\n",
       " '> > Buck Buckner, P.E., MBA > Manager, Business Development and Planning > Big Box Retail Sales > Honeywell Power Systems, Inc. > 8725 Pan American Frwy > Albuquerque, NM 87113 > 505-798-6424 > 505-798-6050x > 505-220-4129 > 888/501-3145 >',\n",
       " 'Mr. Buckner, For delivered gas behind San Diego, Enron Energy Services is the appropriate Enron entity.',\n",
       " 'Her phone number is 713-853-7107.',\n",
       " 'Phillip Allen',\n",
       " '1.',\n",
       " 'Although the meeting with Keith, on Wednesday,  was informative the solution of creating a infinitely dynamic consolidated position screen, will be extremely difficult and time consuming.',\n",
       " 'What needs to happen on Monday from 3 - 5 is a effort to design a desired layout for the consolidated position screen, this is critical.',\n",
       " 'I have been involved in most of the meetings and the discussions have been good.',\n",
       " \"We've had three meetings which brought out very different issues from different traders.\",\n",
       " \"We're getting hit with a lot of different requests, many of which appear to be outside the scope of position consolidation.\",\n",
       " 'Customized rows and columns in the position manager (ad hoc rows/columns that add up existing position manager rows/columns).',\n",
       " 'New drill down in the position manager to break out positions by: physical, transport, swaps, options, ...',\n",
       " 'Addition of a curve tab to the position manager to show the real-time values of all curves on which the desk has a position.',\n",
       " 'Ability to split the current position grid to allow daily positions to be shown directly above monthly positions.',\n",
       " 'Each grouped column in the top grid would be tied to a grouped column in the bottom grid.',\n",
       " 'Ability to properly show curve shift for float-for-float deals; determine the appropriate positions to show for each: Gas Daily for monthly index, Physical gas for Nymex, Physical gas for Inside Ferc, Physical gas for Mid market.',\n",
       " 'Ability for TDS to pull valuation results based on a TDS flag instead of using official valuations.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_action_texts[20:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Filters\n",
    "\n",
    "**NOTE**: This section is applicable when we want to achieve high precision i.e. we might miss a few action items from the original list but we will achieve high accuracy on whatever has been retrieved.\n",
    "\n",
    "### First Level:\n",
    "\n",
    "**Filter A**: Check if no. of action words are more than 1 and if length of email sentence is less than 30.\n",
    "\n",
    "\n",
    "### Second Level:\n",
    "\n",
    "**Filter B**: Check if object pronouns or subject pronouns are present in the sentence. The list is specified as below.\n",
    "\n",
    "\n",
    "### Third Level:\n",
    "\n",
    "\n",
    "**Filter C**: Disregard those with negation words.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filters A,B,C:\n",
    "\n",
    "object_pronouns = ['me', 'her', 'him', 'us', 'them']\n",
    "subject_pronouns = ['i', 'we', 'you', 'he', 'she', 'they']\n",
    "negation_words = [\"shouldn't\", \"couldn't\", \"wouldn't\"]\n",
    "\n",
    "filtered_action_texts = []\n",
    "\n",
    "for item in action_texts:\n",
    "    if item[2]>1 and len(item[0].split())<30:\n",
    "        \n",
    "        \n",
    "    \n",
    "        obj_flag =0 \n",
    "        for x in object_pronouns:\n",
    "            if x in item[0].lower().split(): \n",
    "                filtered_action_texts.append(item[0])\n",
    "                obj_flag =1\n",
    "                break\n",
    "        if (obj_flag == 0):\n",
    "            for x in subject_pronouns:\n",
    "                if x in item[0].lower().split():\n",
    "                    filtered_action_texts.append(item[0])\n",
    "                    break\n",
    "\n",
    "final_action_texts = []\n",
    "for item in filtered_action_texts:\n",
    "    #print (item)\n",
    "    count_neg = 0\n",
    "    for x in negation_words:\n",
    "        if x in item.lower().split():\n",
    "            count_neg += 1\n",
    "            continue\n",
    "    if count_neg == 0:\n",
    "        final_action_texts.append(item)\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"1. login:  pallen pw: ke9davis I don't think these are required by the ISP 2.  static IP address IP: 64.216.90.105 Sub: 255.255.255.248 gate: 64.216.90.110 DNS: 151.164.1.8 3.\",\n",
       " \"Follow these steps so you don't misplace these files.\",\n",
       " 'We really need a single point of contact to help drive the trader requirements and help come to a consensus regarding the requirements.',\n",
       " 'We really need a single point of contact to help drive the trader requirements and help come to a consensus regarding the requirements.',\n",
       " 'Can you please make sure he has an active password.',\n",
       " 'This would give you a total loan of $992,000, total cost of $1,232,645 for equity required of $241,000.',\n",
       " 'This would give you a total loan of $992,000, total cost of $1,232,645 for equity required of $241,000.',\n",
       " 'Please get back to me as soon as your schedule permits regarding the site visit and feel free to call at any time.',\n",
       " 'I will follow up with an email and phone call about Cherry Creek.',\n",
       " 'Jeff, I need to see the site plan for Burnet.',\n",
       " 'Please get back to me as soon as your schedule permits regarding the site visit and feel free to call at any time.',\n",
       " 'Phillip, I will call you today to go over this more thoroughly.',\n",
       " 'Please get back to me as soon as your schedule permits regarding the site visit and feel free to call at any time.',\n",
       " 'During this period, you can use Outlook Web Access (OWA) via your= web browser (Internet Explorer 5.0) to read and send mail.',\n",
       " 'Suzanne, Can you give me more details or email  the plan prior to meeting?',\n",
       " 'You will probably need to close the file before you attach to an email.',\n",
       " \"On the gas side, I think Kean/Lay need an update to a table you prepared for me a few months ago, which I've attached..  Can you oblige?\",\n",
       " 'also, what do I need to enter under domain?',\n",
       " \"On the gas side, I think Kean/Lay need an update to a table you prepared for me a few months ago, which I've attached..  Can you oblige?\",\n",
       " 'When you open the file, go to the \"Checkbook\" tab and look at the yellow highlighted items.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_action_texts[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_union\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the predefined actions from the actions.csv file\n",
    "\n",
    "predefined_actions = pd.read_csv('./actions.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in list\n",
    "\n",
    "predefined_actions_list = predefined_actions[0].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1250"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predefined_actions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a balanced dataset for our classification task. Hence select 1250 non-action items \n",
    "# from our list\n",
    "\n",
    "non_actions_list = non_action_texts[:1250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1250"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(non_actions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the labeled numpy array\n",
    "\n",
    "labels = np.array([ 1 if x < 1250 else 0 for x in range(2500) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(predefined_actions_list + non_actions_list)\n",
    "\n",
    "#random.shuffle(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe\n",
    "data = pd.DataFrame({'item':X,'label':labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thus, the additional equity for the improvemen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Do you have Yahoo Messenger or Hear Me turned on?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Please print and save file for me.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Please review and forward to the appropriate E...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Call my little EES buddies to get better under...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                item  label\n",
       "0  Thus, the additional equity for the improvemen...      0\n",
       "1  Do you have Yahoo Messenger or Hear Me turned on?      0\n",
       "2                 Please print and save file for me.      1\n",
       "3  Please review and forward to the appropriate E...      1\n",
       "4  Call my little EES buddies to get better under...      1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the TF-IDF vectorizer to construct feature vector for each piece of text item - both at the word and character level. \n",
    "\n",
    "This effectively functions as our char and word n-gram feature builder from the original items we have.\n",
    "We take n-gram ranges of 1-5 for characters and 2-4 for words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 5),\n",
    "    max_features=30000)\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    ngram_range=(2, 4),\n",
    "    max_features=30000)\n",
    "vectorizer = make_union(word_vectorizer, char_vectorizer, n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_text = data['item']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureUnion(n_jobs=2,\n",
       "             transformer_list=[('tfidfvectorizer-1',\n",
       "                                TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                                decode_error='strict',\n",
       "                                                dtype=<class 'numpy.float64'>,\n",
       "                                                encoding='utf-8',\n",
       "                                                input='content', lowercase=True,\n",
       "                                                max_df=1.0, max_features=30000,\n",
       "                                                min_df=1, ngram_range=(1, 5),\n",
       "                                                norm='l2', preprocessor=None,\n",
       "                                                smooth_idf=True,\n",
       "                                                stop_words=None,\n",
       "                                                strip_accents='unicode',\n",
       "                                                subl...\n",
       "                                                dtype=<class 'numpy.float64'>,\n",
       "                                                encoding='utf-8',\n",
       "                                                input='content', lowercase=True,\n",
       "                                                max_df=1.0, max_features=30000,\n",
       "                                                min_df=1, ngram_range=(2, 4),\n",
       "                                                norm='l2', preprocessor=None,\n",
       "                                                smooth_idf=True,\n",
       "                                                stop_words=None,\n",
       "                                                strip_accents='unicode',\n",
       "                                                sublinear_tf=True,\n",
       "                                                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                tokenizer=None, use_idf=True,\n",
       "                                                vocabulary=None))],\n",
       "             transformer_weights=None, verbose=False)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the vectorizer on all texts\n",
    "vectorizer.fit(all_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split : 75-25%\n",
    "\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(data['item'], data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = vectorizer.transform(train_text)\n",
    "test_features = vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a simple logistic regression classifier for 2-class classification\n",
    "\n",
    "classifier = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(train_features, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9504"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy\n",
    "classifier.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[296,  11],\n",
       "       [ 20, 298]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the confusion matrix\n",
    "confusion_matrix(np.array(test_labels.tolist()),predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95       307\n",
      "           1       0.96      0.94      0.95       318\n",
      "\n",
      "    accuracy                           0.95       625\n",
      "   macro avg       0.95      0.95      0.95       625\n",
      "weighted avg       0.95      0.95      0.95       625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the Precision, Recall, F-1 scores for the test sets\n",
    "\n",
    "print(classification_report(np.array(test_labels.tolist()),predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen how we can leverage a filter-based mechanism in Objective 1 - with a hierarchical schema to enable unsupervised text classification thereby weeding out non-actionable items from our corpus in the data pipeline stage.\n",
    "\n",
    "Finally, when we have some labeled data, it is essential to note that even with a simple classifier like Logistic Regression, we were able to harness the power of character & word n-grams in order to build a reasonable solution to the actionable-item identification problem.\n",
    "\n",
    "Few challenges that still remain are scaling this to an imbalanced real world setting - and assessing its performance against a larger labeled dataset -- the core idea though, is promising!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
